{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96e3f76-251d-4953-bb0e-cbef6dc3e4ac",
   "metadata": {},
   "source": [
    "# Drone auralization example\n",
    "> How to use Acoular for simulating a drone flyby end exporting a stereo wav file\n",
    "- tags: [drone noise, moving source simulation, audio export]\n",
    "- author: Gert Herold\n",
    "- comments: true\n",
    "\n",
    "In addition to its capabilities to detect and analyze sources with implementations of various array methods, \n",
    "Acoular has several tools for synthesizing signals and simulating measurements. \n",
    "This can be helpful when designing an experiment and pre-testing measurement setups and algorithms without having to actually set up the hardware.\n",
    "\n",
    "In this post, we'll be using these tools to simulate the flyby of a multicopter drone as it would be experienced by an arbitrary observer.\n",
    "Instead of the typical microphone array with dozens of microphones, the output will be restricted to just two channels, which shall represent \n",
    "the \"ears\" of a person.\n",
    "\n",
    "For applications of the simulation workflow in an array context, see also [here](https://doi.org/10.1051/aacus/2022052) or [here](https://doi.org/10.2514/6.2023-3818).\n",
    "\n",
    "In the following, we will explore how to:\n",
    "\n",
    " - implement an Acoular-compatible class for generating an artificial multicopter signal\n",
    " - simulate a source which radiates this signal, has a dipole characteristic and flies along a predefined trajectory\n",
    " - export a stereo wav file with a simple auralization of that flyby\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f2053-1aeb-44dd-a374-97f564a2f049",
   "metadata": {},
   "source": [
    "First, we import Acoular, NumPy, and Matplotlib's plotting functionality, and we initialize a random generator that we need later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac8522-1bbf-4727-bd8c-308bc931a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import acoular as ac\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486b3ea-e34a-4713-8756-99744e8a22b8",
   "metadata": {},
   "source": [
    "As Acoular does not feature a specific way to generate drone signals, we define our own signal class for doing that. \n",
    "It is derived from the existing [`SignalGenerator`](https://www.acoular.org/api_ref/generated/generated/acoular.signals.SignalGenerator.html#acoular.signals.SignalGenerator) class. \n",
    "\n",
    "Signals of multicopter drones are usually very recognizable, often featuring strong tonal components caused by the rotors.\n",
    "In reality, the radiated noise depends on a lot of factors, which we will not consider in detail here.\n",
    "For now, it shall be enough to assume the signal to be dependent on the number of rotors (with respective \n",
    "rotational speeds) and the number of blades per rotor. \n",
    "From that, the blade passing frequencies are calculated as dominant components and nicely mixed with \n",
    "some higher harmonics and broadband noise.\n",
    "\n",
    "The new class shall be called `DroneSignalGenerator` and its interface will allow setting a list of \"revolutions per minute\" values for the rotors, implicitly setting the numbers of rotors, and the already mentioned number of blades per rotor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654ec3c-a84d-4e81-835c-12ef560a3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import traits.api to enforce data types in object parameters\n",
    "from traits.api import List, Int\n",
    "\n",
    "class DroneSignalGenerator( ac.SignalGenerator ):\n",
    "    \"\"\"\n",
    "    Class for generating a synthetic multicopter drone signal. \n",
    "    This is just a basic example class for demonstration purposes \n",
    "    with only few settable and some arbitrary fixed parameters.\n",
    "    It is not intended to create perfectly realistic signals.\n",
    "    \"\"\"\n",
    "\n",
    "    # List with rotor speeds (for each rotor independently)\n",
    "    # Default: 1 rotor, 15000 rpm\n",
    "    rpm_list = List([15000,])\n",
    "\n",
    "    # Number of blades per rotor\n",
    "    # Default: 2\n",
    "    num_blades_per_rotor = Int(2)\n",
    "\n",
    "    def signal( self ):\n",
    "        \"\"\"\n",
    "        function that returns the full signal\n",
    "        \"\"\"\n",
    "        # use 1/f² broadband noise as basis for the signal\n",
    "        wn = rng.standard_normal(self.numsamples) # normal distributed values\n",
    "        wnf = np.fft.rfft(wn) # transform to freq domain\n",
    "        wnf /= (np.linspace(0.1,1,len(wnf))*5)**2 # spectrum ~ 1/f²\n",
    "        sig = np.fft.irfft(wnf) # transform to time domain\n",
    "\n",
    "        # vector with all time instances\n",
    "        t = np.arange(self.numsamples, dtype=float) / self.sample_freq\n",
    "\n",
    "        # iterate over all rotors\n",
    "        for rpm in self.rpm_list:\n",
    "            f_base = rpm / 60 # rotor speed in Hz\n",
    "\n",
    "            # randomly set phase of rotor\n",
    "            phase = rng.uniform() * 2*np.pi\n",
    "            \n",
    "            # calculate higher harmonics up to 50 times the rotor speed\n",
    "            for n in np.arange(50)+1:\n",
    "                # if we're looking at a blade passing frequency, make it louder\n",
    "                if n % self.num_blades_per_rotor == 0:\n",
    "                    amp = 1\n",
    "                else:\n",
    "                    amp = 0.2\n",
    "\n",
    "                # exponentially decrease amplitude for higher freqs with arbitrary factor\n",
    "                amp *= np.exp(-n/10)\n",
    "                \n",
    "                # add harmonic signal component to existing signal\n",
    "                sig += amp * np.sin(2*np.pi*n * f_base * t + phase) \n",
    "\n",
    "        # return signal normalized to given RMS value\n",
    "        return sig * self.rms / np.std(sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a53fea-e5df-4c7a-a8f4-75ce41657861",
   "metadata": {},
   "source": [
    "Now we use the newly-defined class to create an object with parameters for a specific drone signal. \n",
    "We choose to have a quadcopter with four rotors.\n",
    "As the simulation should feature the drone flying by, it makes sense for two of the rotors running at a higher speed than the other two, so as to tilt a quadcopter to let it fly in one direction. \n",
    "Moreover, all rotors are chosen to have slightly different rotational speeds to make it a little more realistic (15010 rpm / 14962 rpm and 13536 rpm / 13007 rpm).\n",
    "\n",
    "The `sample_freq` and `numsamples` traits that we set here are inherited from the `SignalGenerator` base class.\n",
    "We use a standard sampling frequency for audio signals of 44'100 Hz here and generate a signal of a little above 10 seconds.\n",
    "The strength of the signal can be set via the (also inherited) `rms` trait, but we just use the default value of 1.0 here, so don't have to specifically set it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360c7c12-ddb0-407d-bb1e-0fa8471518e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of signal\n",
    "t_msm = 10.5 # s\n",
    "# sampling frequency\n",
    "f_sample = 44100 # Hz\n",
    "\n",
    "drone_signal = DroneSignalGenerator(rpm_list = [15010,14962,13536,13007], \n",
    "                                    num_blades_per_rotor = 2, \n",
    "                                    sample_freq = f_sample, \n",
    "                                    numsamples = f_sample*t_msm)\n",
    "\n",
    "# If you're running the example in an interactive environment, you might want\n",
    "# to listen to the pure signal by uncommenting the two following lines:\n",
    "#from IPython.display import Audio\n",
    "#display(Audio(drone_signal.signal(),rate = f_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831e503-9502-4e39-b329-adebb71e9054",
   "metadata": {},
   "source": [
    "The spectrum (PSD) of our signal looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89272a3-48b0-400a-8876-f558183c5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.psd(drone_signal.signal(), \n",
    "        Fs = f_sample,\n",
    "        NFFT = 4096)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8b5cf5-102f-4ba0-99c7-e81c8f12a12b",
   "metadata": {},
   "source": [
    "In addition to the signal, we need characteristics of the source itself, its movement, the environment, and the way it is observed.\n",
    "We start with the \"observer\", i.e. a microphone array with only two microphones where the ears of a standing person might be located. \n",
    "The observer's head is looking mostly towards positive $y$, but also slightly to the side from which the drone will come later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52694ad2-bb1d-4fc2-8775-b90d4f2adf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ac.MicGeom()\n",
    "m.mpos_tot = np.array([[-0.07, 0.07], # x positions, all values in m\n",
    "                       [-0.03, 0.03], # y\n",
    "                       [ 1.7 , 1.7]]) # z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dbcd8-c626-4a54-af11-fe6fb6149f52",
   "metadata": {},
   "source": [
    "Next, the flight path is defined.\n",
    "A trajectory in Acoular is defined via its \"waypoints\" with corresponding times.\n",
    "We can set as many waypoints as we want as a dictionary. \n",
    "\n",
    "Let's say that our drone flies about 10 m above ground, \n",
    "from left to right, and a little in front of the observer.\n",
    "The flight speed is 16 m/s in $x$ direction.\n",
    "The absolute speed will be a little higher, since we slightly and randomly vary the $z$  and $y$ positions from one waypoint to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ea2de4-2ee8-4b0c-84cb-03967000e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_speed = 16 # m/s\n",
    "\n",
    "# 11 seconds trajectory, which is a little more than we have signal for\n",
    "ts = np.arange(12) \n",
    "\n",
    "# Set one waypoint each second, \n",
    "waypoints = { t : ((t-5.5)*flight_speed,       # vary \n",
    "                     6 + rng.uniform(-0.2,0.2), # randomly vary y position up to ±0.2 m around 6 m \n",
    "                    10 + rng.uniform(-0.3,0.3)) # randomly vary z position up to ±0.3 m around 10 m height\n",
    "              for t in ts }\n",
    "\n",
    "traj = ac.Trajectory(points = waypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0aa3e-c4ad-4b8a-a1a9-14463ceca146",
   "metadata": {},
   "source": [
    "Let's plot the trajectory together with the observer positions, as viewed from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b8751-19b2-446a-a8b0-612c8440335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2,(20,3))\n",
    "\n",
    "# plot observer\n",
    "plt.plot(m.mpos[0,:], m.mpos[1,:], 'rx', label = 'observer')\n",
    "\n",
    "# plot trajectory\n",
    "times = np.linspace(0,11,100)\n",
    "xt, yt, zt = traj.location(times)\n",
    "plt.plot(xt, yt, label = 'trajectory')\n",
    "\n",
    "# plot the predefined waypoints\n",
    "xwp, ywp, zwp = zip(*traj.points.values())\n",
    "plt.plot(xwp, ywp, '>', label = 'traj. waypoints')\n",
    "\n",
    "plt.xlabel('$x$ / m')\n",
    "plt.ylabel('$y$ / m')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ad4f0-b473-4d98-af30-f8352822db1d",
   "metadata": {},
   "source": [
    "Compared to the length of the flight path, the two microphones are positioned rather close to each other, so they appear almost as one position in the plot.\n",
    "\n",
    "Now we define an actual source. Many drones exhibit a strong directivity, so using a [`MovingPointSourceDipole`](https://www.acoular.org/api_ref/generated/generated/acoular.sources.MovingPointSourceDipole.html#acoular.sources.MovingPointSourceDipole) seems a good choice here. \n",
    "If we don't explicitly specify otherwise, the dipole lobes will be oriented along the z axis, which is what we want in this case.\n",
    "For calculating the observed sound pressure, i.e. for taking into account the sound travel path source to the observer, the object needs to know what kind of [environment](https://www.acoular.org/api_ref/generated/acoular.environments.html#module-acoular.environments) it will be existing in.\n",
    "In our case, we just define a resting fluid with a speed of sound of 343 m/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e91d7-98de-4df0-8975-4ca57066bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep the environment simple for now: just air at standard conditions with speed of sound 343 m/s\n",
    "e = ac.Environment(c=343.)\n",
    "\n",
    "# Define point source\n",
    "p = ac.MovingPointSourceDipole(signal = drone_signal, # the signal of the source\n",
    "                               trajectory = traj,     # set trajectory\n",
    "                               conv_amp = True,       # take into account convective amplification\n",
    "                               mics = m,              # set the \"array\" with which to measure the sound field\n",
    "                               start = 0.5,           # observation starts 0.5 seconds after signal starts at drone\n",
    "                               env = e)               # the environment the source is moving in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc28650-cf72-4856-80aa-932c29fa05da",
   "metadata": {},
   "source": [
    "With this, we defined our source. But for a little more realism, let's add a \"mirror source\" to simulate ground reflections. \n",
    "The properties of this source are exactly the same as for the original source, except for the $z$ coordinate, which is inversed here.\n",
    "After the definition of our mirror source, both source are combined into a joint sound field description via a [`SourceMixer`](https://www.acoular.org/api_ref/generated/generated/acoular.sources.SourceMixer.html#acoular.sources.SourceMixer) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5850bb-14cb-425f-bdee-9d5ee3b698a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the waypoints from the original source into a new trajectory, but with inverted z\n",
    "waypoints_reflection = { time : (x, y, -z) for time, (x, y, z) in waypoints.items() }\n",
    "traj_reflection = ac.Trajectory(points = waypoints_reflection)\n",
    "\n",
    "# Define a mirror source with the mirrored trajectory\n",
    "p_reflection = ac.MovingPointSourceDipole(signal = drone_signal,        # the same signal as above\n",
    "                                          trajectory = traj_reflection, # set trajectory of mirror source\n",
    "                                          conv_amp = True,\n",
    "                                          mics = m,\n",
    "                                          start = 0.5,\n",
    "                                          env = e) \n",
    "\n",
    "# Mix the original source and the mirror source\n",
    "drone_above_ground = ac.SourceMixer( sources = [p, p_reflection] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f5757-4b96-4f9d-b292-fb9c87c5e762",
   "metadata": {},
   "source": [
    "Now we export a wav file of our simulation, so that we can listen to it with any audio player software.\n",
    "Because of the lazy evaluation paradigm, all the code above should run rather quickly, since no serious calculations were done up to now.\n",
    "This will change here, as the source signal is propagated sample-per-sample to generate the observed sound pressure time signals.\n",
    "The export may take a few minutes, so don't be alarmed if you don't immediately get the output.\n",
    "\n",
    "Before letting the data stream be exported as wav file, however, we divert it through a [`TimeCache`](https://www.acoular.org/api_ref/generated/generated/acoular.tprocess.TimeCache.html#acoular.tprocess.TimeCache) object, which already writes the data on the disk (in Acoular-compatible format).\n",
    "The reason for this is that we do not need to recalculate everything if we happen to need the exact same data at a later point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3d2a1-d7be-4e9c-b442-ce0a645e1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# Write data stream onto disk for later re-use. This step is not necessary if runtime isn't an issue.\n",
    "cached_signals = ac.TimeCache(source = drone_above_ground)\n",
    "\n",
    "# Prepare wav output.\n",
    "# If you don't need caching, you can directly put \"source = drone_above_ground\" here.\n",
    "output = ac.WriteWAV(name = 'drone_flyby_with_ground_reflection.wav',\n",
    "                     source = cached_signals, \n",
    "                     channels = [0,1]) # export both channels as stereo\n",
    "\n",
    "# Start the actual export\n",
    "output.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0736d4-8519-443c-b555-2556440f2efe",
   "metadata": {},
   "source": [
    "That's it, we did a simple drone auralization using the tools available in Acoular.\n",
    "If you listen to the output file with properly connected stereo headphones, you should hear something that resembles a quadcopter moving from left to right.\n",
    "\n",
    "Finally, let's visualize the transient signal of one channel in a spectrogram. \n",
    "We use Acoular's [`return_result`](https://www.acoular.org/api_ref/generated/generated/generated/acoular.tools.helpers.return_result.html#acoular.tools.helpers.return_result) function on the output object to get the whole signal track at once instead of from a block-wise yielding generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0404263-b357-42df-b388-97468a7bdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3,(15,5))\n",
    "plt.specgram(ac.tools.return_result(output)[:,0], \n",
    "             Fs = f_sample,\n",
    "             noverlap = 4096-256,\n",
    "             NFFT = 4096,\n",
    "             vmin=-100,\n",
    "             vmax=-50)\n",
    "plt.ylim(0,5000)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$t$ / s')\n",
    "plt.ylabel('$f$ / Hz')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e65852-bd7f-4e4b-b3dd-989a79a41807",
   "metadata": {},
   "source": [
    "Although this simulation is comparativly simple and far from realistic, the spectrogram already exhibits many of the features observed in actual measurement data of drone flybys: the frequency shift due to the Doppler effect, higher levels when the drone is close to the observer, and interference patterns due to ground reflections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "ac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
