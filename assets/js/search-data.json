{
  
    
        "post0": {
            "title": "Drone auralization example",
            "content": "In addition to its capabilities to detect and analyze sources with implementations of various array methods, Acoular has several tools for synthesizing signals and simulating measurements. This can be helpful when designing an experiment and pre-testing measurement setups and algorithms without having to actually set up the hardware. . In this post, we&#39;ll be using these tools to simulate the flyby of a multicopter drone as it would be experienced by an arbitrary observer. Instead of the typical microphone array with dozens of microphones, the output will be restricted to just two channels, which shall represent the &quot;ears&quot; of a person. . For applications of the simulation workflow in an array context, see also here or here. . In the following, we will explore how to: . implement an Acoular-compatible class for generating an artificial multicopter signal | simulate a source which radiates this signal, has a dipole characteristic and flies along a predefined trajectory | export a stereo wav file with a simple auralization of that flyby | . First, we import Acoular, NumPy, and Matplotlib&#39;s plotting functionality . import acoular as ac import numpy as np import matplotlib.pyplot as plt . As Acoular does not feature a specific way to generate drone signals, we define our own signal class for doing that. It is derived from the existing SignalGenerator class. . Signals of multicopter drones are usually very recognizable, often featuring strong tonal components caused by the rotors. In reality, the radiated noise depends on a lot of factors, which we will not consider in detail here. For now, it shall be enough to assume the signal to be dependent on the number of rotors (with respective rotational speeds) and the number of blades per rotor. From that, the blade passing frequencies are calculated as dominant components and nicely mixed with some higher harmonics and broadband noise. . The new class shall be called DroneSignalGenerator and its interface will allow setting a list of &quot;revolutions per minute&quot; values for the rotors, implicitly setting the numbers of rotors, and the already mentioned number of blades per rotor. . from traits.api import List, Int class DroneSignalGenerator( ac.SignalGenerator ): &quot;&quot;&quot; Class for generating a synthetic multicopter drone signal. This is just a basic example class for demonstration purposes with only few settable and some arbitrary fixed parameters. It is not intended to create perfectly realistic signals. &quot;&quot;&quot; # List with rotor speeds (for each rotor independently) # Default: 1 rotor, 15000 rpm rpm_list = List([15000,]) # Number of blades per rotor # Default: 2 num_blades_per_rotor = Int(2) def signal( self ): &quot;&quot;&quot; function that returns the full signal &quot;&quot;&quot; # initialize a random generator for noise generation rng = np.random.default_rng(seed = 42) # use 1/f² broadband noise as basis for the signal wn = rng.standard_normal(self.numsamples) # normal distributed values wnf = np.fft.rfft(wn) # transform to freq domain wnf /= (np.linspace(0.1,1,len(wnf))*5)**2 # spectrum ~ 1/f² sig = np.fft.irfft(wnf) # transform to time domain # vector with all time instances t = np.arange(self.numsamples, dtype=float) / self.sample_freq # iterate over all rotors for rpm in self.rpm_list: f_base = rpm / 60 # rotor speed in Hz # randomly set phase of rotor phase = rng.uniform() * 2*np.pi # calculate higher harmonics up to 50 times the rotor speed for n in np.arange(50)+1: # if we&#39;re looking at a blade passing frequency, make it louder if n % self.num_blades_per_rotor == 0: amp = 1 else: amp = 0.2 # exponentially decrease amplitude for higher freqs with arbitrary factor amp *= np.exp(-n/10) # add harmonic signal component to existing signal sig += amp * np.sin(2*np.pi*n * f_base * t + phase) # return signal normalized to given RMS value return sig * self.rms / np.std(sig) . Now we use the newly-defined class to create an object with parameters for a specific drone signal. We choose to have a quadcopter with four rotors. As the simulation should feature the drone flying by, it makes sense for two of the rotors running at a higher speed than the other two, so as to tilt a quadcopter to let it fly in one direction. Moreover, all rotors are chosen to have slightly different rotational speeds to make it a little more realistic (15010 rpm / 14962 rpm and 13536 rpm / 13007 rpm). . The sample_freq and numsamples traits that we set here are inherited from the SignalGenerator base class. We use a standard sampling frequency for audio signals of 44&#39;100 Hz here and generate a signal of a little above 10 seconds. The strength of the signal can be set via the (also inherited) rms trait, but we just use the default value of 1.0 here, so don&#39;t have to specifically set it. . t_msm = 10.5 # s # sampling frequency f_sample = 44100 # Hz drone_signal = DroneSignalGenerator(rpm_list = [15010,14962,13536,13007], num_blades_per_rotor = 2, sample_freq = f_sample, numsamples = f_sample*t_msm) # If you&#39;re running the example in an interactive environment, you might want # to listen to the pure signal by uncommenting the two following lines: #from IPython.display import Audio #display(Audio(drone_signal.signal(),rate = f_sample)) . The spectrum (PSD) of our signal looks like this: . plt.figure(1,(10,3)) plt.psd(drone_signal.signal(), Fs = f_sample, NFFT = 4096) plt.show() . In addition to the signal, we need characteristics of the source itself, its movement, the environment, and the way it is observed. We start with the &quot;observer&quot;, i.e. a microphone array with only two microphones where the ears of a standing person might be located. The observer&#39;s head is looking mostly towards positive $y$, but also slightly to the side from which the drone will come later. . m = ac.MicGeom() m.mpos_tot = np.array([[-0.07, 0.07], # x positions, all values in m [-0.03, 0.03], # y [ 1.7 , 1.7]]) # z . Next, the flight path is defined. A trajectory in Acoular is defined via its &quot;waypoints&quot; with corresponding times. We can set as many waypoints as we want as a dictionary. . Let&#39;s say that our drone flies about 10 m above ground, from left to right, and a little in front of the observer. The flight speed is 16 m/s in $x$ direction. The absolute speed will be a little higher, since we slightly and randomly vary the $z$ and $y$ positions from one waypoint to the next. . flight_speed = 16 # m/s # 11 seconds trajectory, which is a little more than we have signal for ts = np.arange(12) # initialize a random generator for path deviations rng = np.random.default_rng(seed = 23) # Set one waypoint each second, waypoints = { t : ((t-5.5)*flight_speed, # vary 6 + rng.uniform(-0.2,0.2), # randomly vary y position up to ±0.2 m around 6 m 10 + rng.uniform(-0.3,0.3)) # randomly vary z position up to ±0.3 m around 10 m height for t in ts } traj = ac.Trajectory(points = waypoints) . Let&#39;s plot the trajectory together with the observer positions, as viewed from above. . plt.figure(2,(10,3)) # plot observer plt.plot(m.mpos[0,:], m.mpos[1,:], &#39;rx&#39;, label = &#39;observer&#39;) # plot trajectory times = np.linspace(0,11,100) xt, yt, zt = traj.location(times) plt.plot(xt, yt, label = &#39;trajectory&#39;) # plot the predefined waypoints xwp, ywp, zwp = zip(*traj.points.values()) plt.plot(xwp, ywp, &#39;&gt;&#39;, label = &#39;traj. waypoints&#39;) plt.xlabel(&#39;$x$ / m&#39;) plt.ylabel(&#39;$y$ / m&#39;) plt.legend() plt.axis(&#39;equal&#39;) plt.show() . Compared to the length of the flight path, the two observer microphones are positioned rather close to each other, so they appear almost as one position in the plot. . Now we define an actual source. Many drones exhibit a strong directivity, so using a MovingPointSourceDipole seems a good choice here. If we don&#39;t explicitly specify otherwise, the dipole lobes will be oriented along the z axis, which is what we want in this case. For calculating the observed sound pressure, i.e. for taking into account the sound travel path source to the observer, the object needs to know what kind of environment it will be existing in. In our case, we just define a resting fluid with a speed of sound of 343 m/s. . e = ac.Environment(c=343.) # Define point source p = ac.MovingPointSourceDipole(signal = drone_signal, # the signal of the source trajectory = traj, # set trajectory conv_amp = True, # take into account convective amplification mics = m, # set the &quot;array&quot; with which to measure the sound field start = 0.5, # observation starts 0.5 seconds after signal starts at drone env = e) # the environment the source is moving in . With this, we defined our source. But for a little more realism, let&#39;s add a &quot;mirror source&quot; to simulate ground reflections. The properties of this source are exactly the same as for the original source, except for the $z$ coordinate, which is inversed here. After the definition of our mirror source, both source are combined into a joint sound field description via a SourceMixer object. . waypoints_reflection = { time : (x, y, -z) for time, (x, y, z) in waypoints.items() } traj_reflection = ac.Trajectory(points = waypoints_reflection) # Define a mirror source with the mirrored trajectory p_reflection = ac.MovingPointSourceDipole(signal = drone_signal, # the same signal as above trajectory = traj_reflection, # set trajectory of mirror source conv_amp = True, mics = m, start = 0.5, env = e) # Mix the original source and the mirror source drone_above_ground = ac.SourceMixer( sources = [p, p_reflection] ) . Now we export a wav file of our simulation, so that we can listen to it with any audio player software. Because of the lazy evaluation paradigm, all the code above should run rather quickly, since no serious calculations were done up to now. This will change here, as the source signal is propagated sample-per-sample to generate the observed sound pressure time signals. The export may take a few minutes, so don&#39;t be alarmed if you don&#39;t immediately get the output. . Before letting the data stream be exported as wav file, however, we divert it through a TimeCache object, which already writes the data on the disk (in Acoular-compatible format). The reason for this is that we do not need to recalculate everything if we happen to need the exact same data at a later point in time. . cached_signals = ac.TimeCache(source = drone_above_ground) # Prepare wav output. # If you don&#39;t need caching, you can directly put &quot;source = drone_above_ground&quot; here. output = ac.WriteWAV(name = &#39;drone_flyby_with_ground_reflection.wav&#39;, source = cached_signals, channels = [0,1]) # export both channels as stereo # Start the actual export output.save() . That&#39;s it, we did a simple drone auralization using the tools available in Acoular. If you listen to the output file with properly connected stereo headphones, you should hear something that resembles a quadcopter moving from left to right. . Finally, let&#39;s visualize the transient signal of one channel in a spectrogram. We use Acoular&#39;s return_result function on the output object to get the whole signal track at once instead of from a block-wise yielding generator. . plt.figure(3,(10,5)) plt.specgram(ac.tools.return_result(output)[:,0], Fs = f_sample, noverlap = 4096-256, NFFT = 4096, vmin=-100, vmax=-50) plt.ylim(0,5000) plt.colorbar() plt.xlabel(&#39;$t$ / s&#39;) plt.ylabel(&#39;$f$ / Hz&#39;) plt.show() . Although this simulation is comparativly simple and far from realistic, the spectrogram already exhibits many of the features observed in actual measurement data of drone flybys: the frequency shift due to the Doppler effect, higher levels when the drone is close to the observer, and interference patterns due to ground reflections. .",
            "url": "https://blog.acoular.org/2024/09/23/drone-auralization-example.html",
            "relUrl": "/2024/09/23/drone-auralization-example.html",
            "date": " • Sep 23, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "How to import your data into Acoular",
            "content": "Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array which is stored in an HDF5 file. This blog post explains how to convert data available in other formats into this file format. As examples for other file formats we will use both .csv (comma separated text files) and .mat (Matlab files). . To demonstrate how to import and convert the data, we first need to get some data. In our case we use data from Zenodo, where a 64 channel recording of a scene with three sources is available in a number different formats. We use Python&#39;s urllib for the download. Depending on your internet speed this may take a while: . import urllib.request url = &#39;https://zenodo.org/record/5809069/files/&#39; for filename in (&#39;three_sources.h5&#39;,&#39;three_sources.csv&#39;, &#39;three_sourcesv7.mat&#39;,&#39;three_sourcesv73.mat&#39;): urllib.request.urlretrieve(url+filename, filename) . Now we have the same data in four different formats: Acoular&#39;s HDF5, .csv, Matlab version &lt;= 7 and Matlab versions &gt;=7.3. . HDF5 format is an open all purpose numerical data container file format. Data objects inside HDF5 files are stored in tree-like structure comparable to files and folders in a file system. Lets open the file and explore this structure, which is very simple in this case. . We use the pytables library to access the file. This the very same library used by Acoular under the hood. Alternatively Acoular can also work with h5py. . You could also use an HDF5 file viewer with a GUI (e.g. HDFView). . import tables h5file = tables.open_file(&#39;three_sources.h5&#39;, mode = &#39;r&#39;) # read only mode h5file.root . / (RootGroup) &#39;&#39; children := [&#39;time_data&#39; (EArray)] . In its root sits just one object (one &#39;child&#39;), which is an EArray (extensible array). Lets inspect its properties: . h5file.root.time_data . /time_data (EArray(51200, 64)) &#39;&#39; atom := Float32Atom(shape=(), dflt=0.0) maindim := 0 flavor := &#39;numpy&#39; byteorder := &#39;little&#39; chunkshape := (256, 64) . We see that this array has the size of 51200 (samples) by 64 (channels). The values are stored as 32 bit float numbers. While less than the usual 64 bit, 32 bit accuracy is more than enough in this case and it saves file space. The data itself can be accessed just like for a numpy array. As an example, we read the first 10 samples of channel 47. . h5file.root.time_data[:10,47] . array([ 1.5875906 , -0.7917087 , 3.1555338 , 1.0036362 , -3.1655273 , -6.466202 , -0.19289835, 1.7383114 , 6.901536 , 2.723017 ], dtype=float32) . Along with the data itself, the object stores also some metadata (&#39;attributes&#39;). . h5file.root.time_data.attrs . /time_data._v_attrs (AttributeSet), 5 attributes: [CLASS := &#39;EARRAY&#39;, EXTDIM := 0, TITLE := &#39;&#39;, VERSION := &#39;1.1&#39;, sample_freq := 51200.0] . There is one custom attribute here, which is sample_freq. It specifies the sampling frequency. In our case 51200.0 Hz. . If we now have data in some other format that we want to use with Acoular, there are two options: . We read that data and convert it into an HDF5 file that follows the specification explained. This is demonstrated in this blog post. | We extend Acoular to read the other file format directly. This would mean to subclass the TimeSamples class and requires some understanding of Acoular&#39;s code and working mechanism. | The first option shall now be demonstrated using .csv formatted data. Despite beeing extra inefficient this human-readable text format is widely used. The file contains the same number of floating point numbers separated by commas on each line. Some .csv files have also one or more header lines explaining the data contained in the file. In our case there are no header lines. There are multiple options how to read such file into Python. We are going to use Numpy for this. Be warned, the import of this (relatively small) 80 MByte file takes some time. . import numpy as np datacsv = np.genfromtxt(&#39;three_sources.csv&#39;, delimiter=&#39;,&#39;, dtype=&#39;float32&#39;) datacsv . array([[-0.43654928, -4.696499 , -2.9038546 , ..., -0.39481497, -3.7462494 , -3.2238567 ], [ 2.2970407 , -1.9746966 , -4.089035 , ..., -3.8922982 , -4.8707275 , -3.613382 ], [-2.261127 , 1.6419717 , 3.4066103 , ..., -0.732125 , 0.22087638, -1.6310387 ], ..., [-1.530854 , -1.2453959 , 1.566295 , ..., -3.9039657 , -0.00989423, -6.0220094 ], [ 0.47992265, 3.8888328 , -0.15509878, ..., -1.2525555 , -2.5308452 , -3.22349 ], [-1.0162828 , 1.230733 , -2.4700263 , ..., -5.659823 , -5.2780933 , -0.36301124]], dtype=float32) . Now the data is stored in the datacsv array. the next step is to create a new HFD5 file, store the data into that file and add the attribute for the sampling frequency. . h5filecsv = tables.open_file(&#39;three_sources_from_csv.h5&#39;, mode=&#39;w&#39;, title=&#39;three_sources&#39;) earraycsv = h5filecsv.create_earray(&#39;/&#39;, &#39;time_data&#39;, obj=datacsv) display(earraycsv) h5filecsv.root.time_data.set_attr(&#39;sample_freq&#39;,51200.0) h5filecsv.close() . /time_data (EArray(51200, 64)) &#39;&#39; atom := Float32Atom(shape=(), dflt=0.0) maindim := 0 flavor := &#39;numpy&#39; byteorder := &#39;little&#39; chunkshape := (256, 64) . Just as before with the original HDF5 file we now have the data in new HDF5 file that could be used as data source for Acoular. There is one possible pitfall with this approach: the data is completely read into the computer memory before beeing stored into the HDF5 file. If the data is really huge, say hundreds of channels and some minutes of recording, it might not fit into the memory. In this case, a more sophisticated approach is needed, where chunks of data are read and stored consecutively. Because we use an EArray, this is possible, but we would have to modify the code. . As mentioned before, there are other options to read the .csv data. One that deserves to be mentioned here is Pandas which reads a lot of different data formats. . For some reason it is quite popular to store data in the format used by Matlab. However, it is important to know that despite the same extension (.mat), there are different formats. If we have any of the formats used prior to Matlab v7, then we can use Scipy to import this: . from scipy.io import loadmat ans = loadmat(&#39;three_sourcesv7.mat&#39;)[&#39;ans&#39;] datamat7 = np.array(ans, dtype=&#39;float32&#39;) h5filemat7 = tables.open_file(&#39;three_sources_from_mat7.h5&#39;, mode=&#39;w&#39;, title=&#39;three_sources&#39;) earraymat7 = h5filemat7.create_earray(&#39;/&#39;, &#39;time_data&#39;, obj=datamat7) display(earraymat7) h5filemat7.root.time_data.set_attr(&#39;sample_freq&#39;,51200.0) h5filemat7.close() . /time_data (EArray(51200, 64)) &#39;&#39; atom := Float32Atom(shape=(), dflt=0.0) maindim := 0 flavor := &#39;numpy&#39; byteorder := &#39;little&#39; chunkshape := (256, 64) . The format of .mat file from version 7.3 onwards is essentially an HDF5 file itself! It just uses another file name extension. Of course the internal structure is different from what Acoular is using. However, we can open it with pytables and read the data in it. . matfile73 = tables.open_file(&#39;three_sourcesv73.mat&#39;, mode = &#39;r&#39;) # be aware of Matlab transposing the array here datamat73 = np.array(matfile73.root.ans[:,:], dtype=&#39;float32&#39;).T h5filemat73 = tables.open_file(&#39;three_sources_from_mat73.h5&#39;, mode=&#39;w&#39;, title=&#39;three_sources&#39;) earraymat73 = h5filemat73.create_earray(&#39;/&#39;, &#39;time_data&#39;, obj=datamat73) display(earraymat73) h5filemat73.root.time_data.set_attr(&#39;sample_freq&#39;,51200.0) h5filemat73.close() . /time_data (EArray(51200, 64)) &#39;&#39; atom := Float32Atom(shape=(), dflt=0.0) maindim := 0 flavor := &#39;numpy&#39; byteorder := &#39;little&#39; chunkshape := (256, 64) . This blog post has demostrated how to import data from foreign formats into Acoular. It can also be used as a guide how to convert any other formats not explicitly mentioned here. .",
            "url": "https://blog.acoular.org/2022/06/24/convert-input-data.html",
            "relUrl": "/2022/06/24/convert-input-data.html",
            "date": " • Jun 24, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Getting started with Acoular - Part 3",
            "content": "This is the third and final in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first two posts and continues by explaining additional concepts to be used with time domain methods. . Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. . To continue, we do the same set up as in Part 1. However, as we are setting out to do some signal processing in time domain, we define only TimeSamples, MicGeom, RectGrid and SteeringVector objects but no PowerSpectra or BeamformerBase. . import acoular ts = acoular.TimeSamples( name=&quot;three_sources.h5&quot; ) mg = acoular.MicGeom( from_file=&quot;array_64.xml&quot; ) rg = acoular.RectGrid( x_min=-0.2, x_max=0.2, y_min=-0.2, y_max=0.2, z=0.3, increment=0.01 ) st = acoular.SteeringVector( grid=rg, mics=mg ) . For processing in time domain in Acoular, one may set up &quot;chains&quot; of processing blocks. This is very flexible and allows for easy implementation of new algorithms or algorithmic steps. Each of the blocks acts on all channels at once. Input and output may have different numbers of channels. . For our task we set up the following processing chain: . data intake from file (TimeSamples, same as before) | beamforming. In the time domain this amounts to different delays that have to be applied to all channels and for all grid points, and a sum for each of the grid points. This is also known as delay-and-sum. | band pass filtering (the time history for each point in the map is filtered). We could skip that step in principle, but it is nice to compare the result to what we got in Parts 1 and 2 from frequency domain processing, where we did a similar approach to band pass filtering. | power estimation (just the square, nothing else), so that we can compute levels | linear average over consecutive blocks in time which makes it possible to have not one image for every sample in time, which is huge amount of (mostly useless) data, but just enough data for some images | Each object in the processing chain is connected to its predecessor via the source parameter: . bt = acoular.BeamformerTime( source=ts, steer=st ) ft = acoular.FiltOctave( source=bt, band=8000, fraction=&#39;Third octave&#39; ) pt = acoular.TimePower( source=ft ) avgt = acoular.TimeAverage( source=pt, naverage=6400 ) . And again: lazy evaluation, nothing is computed yet. . Only asking for the result will initiate computing. Although this is not used in this example, it should be mentioned that the architecture allows for endless data processing from a stream of input data. To this end it is possible to replace the TimeSamples object that reads the data not from a file, but from hardware. . Different to the frequency domain processing, the result is not computed in one go, but in blocks of data. These blocks have a variable length that can be defined as argument of the result function each of the processing blocks have. Note that this function is actually a Python generator, that yields a number of results we have to iterate over. This helps if one wants to process large amounts of data that do not fit into the memory at once. Iteration means we can use it in for loop and get a new data block each time we run through the loop. . In our example we use the loop in a Python list comprehension statement. That means we collect all blocks into a list. In our case, the blocks have length 1, i.e. one map per block. . res = [r.copy() for r in avgt.result(1)] . The list res contains all maps each of which averages over 6400 samples. Now we can plot all of these maps. Because time domain processing sees the map as (number of gridpoints) channels, we have to reshape the maps so that fit the shape of the grid. . %matplotlib notebook import matplotlib.pyplot as plt plt.figure(figsize=(10,7)) for i,r in enumerate(res): pm = r[0].reshape(rg.shape) Lm = acoular.L_p(pm) plt.subplot(2,4,i+1) plt.imshow(Lm.T, vmin=Lm.max()-15, origin=&#39;lower&#39;, extent=rg.extend()) plt.title(&#39;sample %i to %i&#39; % (i*6400,i*6400+6399)) plt.tight_layout(); . Because the sources emit a stationary signal, the individual maps do look not much different. The result is also very similar to what we got with the frequency domain beamformer in Part 1. . We can assemble the processing chain also in a different way with positions 2 and 3 exchanged: . data intake | band pass filtering | beamforming | power estimation | linear average | In this case we have to be careful about the effects of filtering: Because the band pass filter comes also with a frequency dependent delay, this can disturb the work of the beamformer in case that sources are not stationary. To circumvent this, we could use a special filter FiltFiltOctave with zero delay. The disavantage of this filter is that the whole time history must be read into the memory, before the first block can be processed by the beamformer. In the present simple example it is not necessary to do this. . We just &quot;rewire&quot; the processing chain. One advantage in this case is that we only have to band pass filter 64 channels (number of microphones) instead of 1641 channels (number of grid points as in the first case. . ft.source = ts bt.source = ft pt.source = bt . After the new chain is set up, we can again plot the results. This time we do not use an extra list for all results, but iterate directly over the maps while plotting. Remember that this takes some time. . plt.figure(figsize=(10,7)) for i,r in enumerate(avgt.result(1)): pm = r[0].reshape(rg.shape) Lm = acoular.L_p(pm) plt.subplot(2,4,i+1) plt.imshow(Lm.T, vmin=Lm.max()-15, origin=&#39;lower&#39;, extent=rg.extend()) plt.title(&#39;sample %i to %i&#39; % (i*6400,i*6400+6399)) plt.tight_layout(); . According to our expectations, the result looks very much the same with this alternative processing chain. . This concludes this series of three blog posts about first steps in Acoular. If you want to know more, look at the documentation and the examples and the reference you will find there or watch out for new blog posts to come! .",
            "url": "https://blog.acoular.org/2021/04/01/getstart3.html",
            "relUrl": "/2021/04/01/getstart3.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Getting started with Acoular - Part 2",
            "content": "This is the second in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first post and continues by explaining some more concepts and additional methods. . Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. . To continue, we do the same set up as in Part 1. We define TimeSamples, PowerSpectra, MicGeom, RectGrid and SteeringVector objects and set up a BeamformerBase. . import acoular ts = acoular.TimeSamples( name=&quot;three_sources.h5&quot; ) ps = acoular.PowerSpectra( time_data=ts, block_size=128, window=&quot;Hanning&quot; ) mg = acoular.MicGeom( from_file=&quot;array_64.xml&quot; ) rg = acoular.RectGrid( x_min=-0.2, x_max=0.2, y_min=-0.2, y_max=0.2, z=0.3, increment=0.01 ) st = acoular.SteeringVector( grid=rg, mics=mg ) bb = acoular.BeamformerBase( freq_data=ps, steer=st ) . We can now plot the result the same way as we already did in Part 1. . %matplotlib notebook import matplotlib.pyplot as plt Lm = acoular.L_p( bb.synthetic(8000,3) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . [(&#39;three_sources_cache.h5&#39;, 1)] . The result is obviously the same as before. . It was computed using a variant of the beamformer that applies the diagonal removal technique. This setting is the default and it means that we ignore the information in the main diagonal of the CSM, where the auto spectra (self-cross spectra) are stored. This is not too harmful in most cases and has the advantage that the map contains less artifacts. After this explanation, we of course now want to see the result with the full CSM including the diagonal (and hopefully with those artifacts). . We achieve this by setting the corresponding flag of the bb object. Have a look at the documentation on BeamformerBase to understand the r_diag flag. . bb.r_diag = False . Obviously nothing happens (right away). This is due to the lazy evaluation again. To get the result we again have to explicitly ask for it and then we can again plot it. . Lm = acoular.L_p( bb.synthetic(8000,3) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . Nice! But indeed there are some artifacts that we may have mistaken for additional weak sound sources if we did not know that there are exactly three sound sources. . Changes in other objects will also affect the beamformer. To try this out, we change the file name in the ts object. Now this object gets data from new time histories. The ps and bb objects &quot;know&quot; about this automatically. No need to inform these other objects about the change. We can immediately ask for the new result and it will be computed. . ts.name=&quot;two_sources.h5&quot; Lm = acoular.L_p( bb.synthetic( 8000, 3 ) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . [(&#39;two_sources_cache.h5&#39;, 1)] . There are only two sources. The result is different, obviously! . Now let us try a different type of beamformer. Instead of standard beamforming, we use functional beamforming. We set up a new object bf and specify the $ gamma$ parameter which is specific to that type of beamformer. We also use the same time histories for the three sources scene as before. . ts.name=&quot;three_sources.h5&quot; bf = acoular.BeamformerFunctional( freq_data=ps, steer=st, gamma=50 ) Lm = acoular.L_p( bf.synthetic(8000,3) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . [(&#39;two_sources_cache.h5&#39;, 1), (&#39;three_sources_cache.h5&#39;, 1)] . Note the much smaller lobes of this beamformer. In other words, the image is less blurry. . Instead of just beamforming we can apply a deconvolution method. Deconvolution methods aim to remove all blur and artifacts from the result. The original blurry beamforming result can be understood as the convolution of the ideal image with a filter kernel (the point spread function) which is specific to the beamforming algorithm. Deconvolution is any technique that attempts to reverse this convolution. . In Acoular the deconvolution is achieved in a similar way to beamforming. Instead of defining a BeamformerBase type of object, we have to use a &quot;beamformer&quot; with deconvolution. In our example we use the CleanSC deconvolution method and BeamformerCleansc type of object. Despite the python class name we chose it is not just a beamformer. . ts.name=&quot;three_sources.h5&quot; bs = acoular.BeamformerCleansc( freq_data=ps, steer=st ) Lm = acoular.L_p( bs.synthetic( 8000, 3) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . [(&#39;two_sources_cache.h5&#39;, 1), (&#39;three_sources_cache.h5&#39;, 2)] . Depending on the computer, this computation may take some seconds already. Now there is just one grid &quot;point&quot; per source and the image looks perfect. . You may have noticed that whenever a results is computed, some messages about cache files appear. This happens because all computed results are cached to a file on disk. If one ask for the same result twice, it is still computed only once to save computing time and effort. This even means at the next start of this notebook no recalculation is necessary and all results are immediately available! . We can try out the automatic caching mechanism by setting up another BeamformerCleansc object bs1 with all the same parameters as the bs object we did already use. . bs1 = acoular.BeamformerCleansc( freq_data=ps, steer=st ) print( bs, bs1 ) . &lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945f042290&gt; &lt;acoular.fbeamform.BeamformerCleansc object at 0x7f945efdbcb0&gt; . Note that while we have two identical objects, they are not the same object. Thus, the bs1 does not know about the results while bs still does. If we now trigger the computation of the results for bs1 they are not actually computed, but just read from the cache. No noteable delay will happen when executing the following commands. . Lm = acoular.L_p( bs1.synthetic(8000,3) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . [(&#39;two_sources_cache.h5&#39;, 1), (&#39;three_sources_cache.h5&#39;, 3)] . This is indeed the same result as before. . However, the cache is not used when parameters are changing. This is desirable because the result may also change. We demonstrate this here by changing one parameter of the bs object. . print( bs.damp ) bs.damp = 0.99 . 0.6 . With the new value, a new computation is necessary. . Lm = acoular.L_p( bs.synthetic( 8000, 3 ) ) plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend()) plt.colorbar(); . This took again some time. In our simple example the result looks not much different, but there was no way to know this before. . In this blog post, we learned how to use alternative methods like functional beamforming and CleanSC. There are many more such methods implemented in Acoular and now you know how to check them out. The last part of this blog post series is about time domain beamforming. .",
            "url": "https://blog.acoular.org/2021/04/01/getstart2.html",
            "relUrl": "/2021/04/01/getstart2.html",
            "date": " • Apr 1, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Getting started with Acoular - Part 1",
            "content": "This is the first in a series of three blog posts about the basic use of Acoular. It explains some fundamental concepts and walks through a simple example. . Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. . To use Acoular, we first have to import Acoular into our notebook. . import acoular . In this example, we want to analyze time histories from 64 microphones stored in the file &quot;three_sources.h5&quot;. The file contains data from a measurement (actually a simulated measurement in this special case) of scene with three different sound sources. . This file is in HDF5 format, which is an open all purpose numerical data container file format. Besides the time histories it contains information about the sampling rate. To learn about the internal structure of the file, have a look at the file! You will need an HDF5 file viewer (e.g. https://www.hdfgroup.org/downloads/hdfview/). For now, we can skip this step and simply load the file into our notebook. The file can be found in the same directory as the notebook itself. . ts = acoular.TimeSamples( name=&quot;three_sources.h5&quot; ) . The file is not directly opened using Python commands, but instead we use the TimeSamples class from Acoular. This class manages the data in the file in an intelligent way which even allows to use huge data files that do not fit into the memory. . If we inspect the ts object, we see not the data itself, but just the type and the location in memory. . ts . &lt;acoular.sources.TimeSamples at 0x7ff0b3e48710&gt; . We can use this object now to answer some questions about the data: . How many channels, | and how many samples do we have? | What is the sampling frequency? | . print( ts.numchannels, ts.numsamples, ts.sample_freq ) . 64 51200 51200.0 . The signal processing can either take place in the time domain or in the frequency domain. To work in the frequency domain, the time history data must be transformed into power spectra. More specifically, we need the cross spectral matrix (CSM) which contains the pairwise cross spectra of all possible combinations of two channels. The cross spectral matrix is computed using Welch&#39;s method. For this, an FFT block size and a window type have to be chosen. . ps = acoular.PowerSpectra( time_data=ts, block_size=128, window=&quot;Hanning&quot; ) ps.fftfreq() . array([ 0., 400., 800., 1200., 1600., 2000., 2400., 2800., 3200., 3600., 4000., 4400., 4800., 5200., 5600., 6000., 6400., 6800., 7200., 7600., 8000., 8400., 8800., 9200., 9600., 10000., 10400., 10800., 11200., 11600., 12000., 12400., 12800., 13200., 13600., 14000., 14400., 14800., 15200., 15600., 16000., 16400., 16800., 17200., 17600., 18000., 18400., 18800., 19200., 19600., 20000., 20400., 20800., 21200., 21600., 22000., 22400., 22800., 23200., 23600., 24000., 24400., 24800., 25200., 25600.]) . We see that after the FFT we do have spectra with 400 Hz frequency spacing. For most applications this would be too coarse. We use it here to get faster processing. If we wish to have finer spacing, we need larger block sizes. . Up to now we have defined how the processing should be done, but we did not compute the actual CSM. Nearly all expensive computations in Acoular are only done on demand using a lazy evaluation paradigm. We can trigger the computation of the cross spectral matrix by just asking for it. In this example we do want to print this already large matrix. Instead we only print its shape. . ps.csm.shape . [(&#39;three_sources_cache.h5&#39;, 1)] . (65, 64, 64) . This matrix actually has the dimensions 65 (number of frequencies) times 64 by 64 (number of microphone channels). . To continue with our task to generate an acoustic photograph, we need the microphone positions. In Acoular, one option is to read them from an XML file. The file looks like this: . &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;MicArray name=&quot;array_64&quot;&gt; &lt;pos Name=&quot;Point 1&quot; x=&quot;0.152&quot; y=&quot;0.1141&quot; z=&quot;0&quot;/&gt; &lt;pos Name=&quot;Point 2&quot; x=&quot;0.134&quot; y=&quot;0.1021&quot; z=&quot;0&quot;/&gt; ... &lt;pos Name=&quot;Point 64&quot; x=&quot;0.0218&quot; y=&quot;-0.0329&quot; z=&quot;0&quot;/&gt; &lt;/MicArray&gt; . (most of the lines are omitted here) . A MicGeom object handles the file: . mg = acoular.MicGeom( from_file=&quot;array_64.xml&quot; ) mg.mpos . array([[ 0.152 , 0.134 , 0.1043, 0.0596, 0.0798, 0.0659, 0.0262, 0.0272, 0. , 0.004 , 0.0162, 0.0162, 0.004 , -0.0112, -0.018 , -0.0112, -0.145 , -0.1294, -0.1242, -0.1209, -0.0828, -0.0631, -0.0595, -0.034 , 0.0056, 0.0037, -0.016 , -0.0492, -0.0024, 0.0022, -0.0267, -0.0054, -0.0874, -0.0764, -0.049 , -0.0058, -0.0429, -0.0378, 0.0003, -0.0121, -0.1864, -0.1651, -0.1389, -0.1016, -0.1008, -0.0809, -0.0475, -0.0369, 0.1839, 0.1634, 0.146 , 0.1235, 0.1019, 0.0799, 0.0594, 0.0393, 0.0774, 0.0697, 0.0778, 0.0944, 0.0473, 0.0338, 0.0478, 0.0218], [ 0.1141, 0.1021, 0.1036, 0.1104, 0.0667, 0.0497, 0.0551, 0.0286, 0. , -0.0175, -0.0078, 0.0078, 0.0175, 0.0141, 0. , -0.0141, 0.1228, 0.1079, 0.0786, 0.0335, 0.0629, 0.0531, 0.0133, 0.0202, 0.1899, 0.1685, 0.1461, 0.1155, 0.104 , 0.0825, 0.0548, 0.0391, -0.1687, -0.1502, -0.1386, -0.1254, -0.0947, -0.0733, -0.061 , -0.0376, -0.0368, -0.0339, -0.0481, -0.0736, -0.0255, -0.0162, -0.0382, -0.014 , -0.0477, -0.0411, -0.0169, 0.0223, -0.0208, -0.0205, 0.0138, -0.0034, -0.1735, -0.1534, -0.1247, -0.0827, -0.0926, -0.0753, -0.0378, -0.0329], [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]) . The mg object now contains all information about the microphone positions. Now let us this to plot the microphone geometry. For plotting we use the matplotlib package. . %matplotlib notebook import matplotlib.pylab as plt plt.plot( mg.mpos[0], mg.mpos[1], &#39;o&#39; ) plt.axis( &#39;equal&#39; ); . This gives a nice impression of the microphone arrangement which consists of 7 inertwined planar logarithmic spirals (hard to see the spirals, I admit). . To map the sound, we need a mapping grid. Here we construct a simple regular and rectangular grid. Note that we have to decide about the size, spacing and the distance (z coordinate) from the array. Size and spacing define the number of points in the grid. The more points in the grid, the better the resolution of the acoustic photograph, but the longer the processing will take. . rg = acoular.RectGrid( x_min=-0.2, x_max=0.2, y_min=-0.2, y_max=0.2, z=0.3, increment=0.01 ) rg.pos() . array([[-0.2 , -0.2 , -0.2 , ..., 0.2 , 0.2 , 0.2 ], [-0.2 , -0.19, -0.18, ..., 0.18, 0.19, 0.2 ], [ 0.3 , 0.3 , 0.3 , ..., 0.3 , 0.3 , 0.3 ]]) . The rg object now has all information about the grid. . The actual method we will use is beamforming. Basically this works by using the combined microphones as a sound receiver with a directivity that is steered to each one of the grid points in turn. One important element in beamforming and similar methods is the steering vector implemented in Acoular in the SteeringVector class. This vector &quot;connects&quot; grid and microphones and takes into account the environmental conditions. These conditions are defined by the speed of sound and a possible background flow. If not set explicitely, a &#39;standard&#39; environment is created in Acoular which assumes quiescent conditions (no flow) and a speed of sound of 343 m/s (air at 20°C). . st = acoular.SteeringVector( grid=rg, mics=mg ) st.env.c . 343.0 . Indeed the standard speed of sound is used. . Now, we define the method we want to use and set up a standard (basic) beamformer. For this need two ingredients: the CSM and the steering vector. . bb = acoular.BeamformerBase( freq_data=ps, steer=st ) . Remember that Acoular uses lazy evaluation. No computation yet! . This means although we set up everything needed to perform beamforming, computation is postponed until the results are actually needed. . This will happen if we ask for the result. In this example we are interested in the sum for of all FFT frequency lines in the 3rd octave band 8000 Hz. This means we need the maps for all these frequencies and then add them together to synthetically &quot;mimic&quot; the result of a third octave filter for that band. The result is given as mean square sound pressure contribution at the center location of the microphone array. In Acoustics, we are usually like to have the results in the form of sound pressure levels (SPL). Acoular has a helper function L_p to compute the levels from the mean square. . pm = bb.synthetic( 8000, 3 ) Lm = acoular.L_p( pm ) . [(&#39;three_sources_cache.h5&#39;, 2)] . The map is now stored in the array variable pm and the array Lm holds the soundpressure levels computed from this. . print(Lm.shape) print(Lm) . (41, 41) [[-350. -350. -350. ... -350. -350. -350. ] [-350. -350. 61.5185086 ... -350. -350. -350. ] [ 66.4266366 65.93676977 68.3501483 ... -350. -350. -350. ] ... [-350. -350. -350. ... -350. -350. -350. ] [-350. 54.71250037 46.46415942 ... 48.39371023 58.52533737 -350. ] [ 58.13562703 62.27400964 61.09820493 ... -350. 61.88124663 61.96166613]] . The map has the same dimensions (41 x 41) as the grid. Any zero result in the map will be clipped to -350 dB level instead of -infinity. . Now lets plot the map as a color-coded image with 15 dB dynamic range between both ends of the color scale. . plt.figure() plt.imshow( Lm.T, origin=&quot;lower&quot;, vmin=Lm.max()-15, extent=rg.extend() ) plt.colorbar(); . Now we enjoy the acoustic photograph of the three sources. Although it looks a bit blurry, we can guess the location of the sources and see as well that the sources have different strength. . This post has demonstrated how to use Acoular to produce a sound map or acoustic photograph with beamforming. Here you can see how to change some parameters and here you can learn about Acoular and time domain processing. .",
            "url": "https://blog.acoular.org/2021/04/01/getstart1.html",
            "relUrl": "/2021/04/01/getstart1.html",
            "date": " • Apr 1, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Acoular",
          "content": "Acoular is a framework for acoustic beamforming that is written in the Python programming language. It is aimed at applications in acoustic testing. Multichannel data recorded by a microphone array can be processed and analyzed in order to generate mappings of sound source distributions. The maps (acoustic photographs) can then be used to locate sources of interest and to characterize them using their spectra. . A few highlights of the framework: . covers several beamforming algorithms | different advanced deconvolution algorithms | both time-domain and frequency-domain operation included | 3D mapping possible | application for stationary and for moving targets | supports both scripting and graphical user interface | efficient: intelligent caching, parallel computing with Numba | easily extendible and well documented | . Learn more: Github / Documentation / Forum .",
          "url": "https://blog.acoular.org/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
      ,"page7": {
          "title": "Tag Index",
          "content": "{% assign date_format = site.date_format | default: “%B %-d, %Y” %} | . {%- capture site_tags -%} {%- for tag in site.tags -%} {{- tag | first -}}{%- unless forloop.last -%},{%- endunless -%} {%- endfor -%} {%- endcapture -%} {%- assign tags_list = site_tags | split:’,’ | sort -%} . {%- for tag in tags_list -%}  {{- tag -}} ({{site.tags[tag].size}}) {%- endfor -%} . {%- for tag in tags_list -%} &nbsp;{{- tag -}}&nbsp;({{site.tags[tag].size}}) . {%- for post in site.tags[tag] -%} {{- post.title -}} {{- post.date | date: date_format -}} {%- endfor -%} {%- endfor -%}",
          "url": "https://blog.acoular.org/tags.html",
          "relUrl": "/tags.html",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.acoular.org/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}