<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://blog.acoular.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.acoular.org/" rel="alternate" type="text/html" /><updated>2024-09-25T09:42:09-05:00</updated><id>https://blog.acoular.org/feed.xml</id><title type="html">Acoular</title><subtitle>A blog on the acoustic testing and source mapping software</subtitle><entry><title type="html">Drone auralization example</title><link href="https://blog.acoular.org/2024/09/23/drone-auralization-example.html" rel="alternate" type="text/html" title="Drone auralization example" /><published>2024-09-23T00:00:00-05:00</published><updated>2024-09-23T00:00:00-05:00</updated><id>https://blog.acoular.org/2024/09/23/drone-auralization-example</id><author><name>Gert Herold</name></author><category term="drone noise" /><category term="moving source simulation" /><category term="audio export" /><summary type="html">In addition to its capabilities to detect and analyze sources with implementations of various array methods, Acoular has several tools for synthesizing signals and simulating measurements. This can be helpful when designing an experiment and pre-testing measurement setups and algorithms without having to actually set up the hardware. In this post, we'll be using these tools to simulate the flyby of a multicopter drone as it would be experienced by an arbitrary observer. Instead of the typical microphone array with dozens of microphones, the output will be restricted to just two channels, which shall represent the &quot;ears&quot; of a person. For applications of the simulation workflow in an array context, see also here or here. In the following, we will explore how to: implement an Acoular-compatible class for generating an artificial multicopter signal simulate a source which radiates this signal, has a dipole characteristic and flies along a predefined trajectory export a stereo wav file with a simple auralization of that flyby First, we import Acoular, NumPy, and Matplotlib's plotting functionality, and we initialize a random generator that we need later on. import acoular as ac import numpy as np import matplotlib.pyplot as plt rng = np.random.default_rng(seed = 42) As Acoular does not feature a specific way to generate drone signals, we define our own signal class for doing that. It is derived from the existing SignalGenerator class. Signals of multicopter drones are usually very recognizable, often featuring strong tonal components caused by the rotors. In reality, the radiated noise depends on a lot of factors, which we will not consider in detail here. For now, it shall be enough to assume the signal to be dependent on the number of rotors (with respective rotational speeds) and the number of blades per rotor. From that, the blade passing frequencies are calculated as dominant components and nicely mixed with some higher harmonics and broadband noise. The new class shall be called DroneSignalGenerator and its interface will allow setting a list of &quot;revolutions per minute&quot; values for the rotors, implicitly setting the numbers of rotors, and the already mentioned number of blades per rotor. from traits.api import List, Int class DroneSignalGenerator( ac.SignalGenerator ): &amp;quot;&amp;quot;&amp;quot; Class for generating a synthetic multicopter drone signal. This is just a basic example class for demonstration purposes with only few settable and some arbitrary fixed parameters. It is not intended to create perfectly realistic signals. &amp;quot;&amp;quot;&amp;quot; # List with rotor speeds (for each rotor independently) # Default: 1 rotor, 15000 rpm rpm_list = List([15000,]) # Number of blades per rotor # Default: 2 num_blades_per_rotor = Int(2) def signal( self ): &amp;quot;&amp;quot;&amp;quot; function that returns the full signal &amp;quot;&amp;quot;&amp;quot; # use 1/f² broadband noise as basis for the signal wn = rng.standard_normal(self.numsamples) # normal distributed values wnf = np.fft.rfft(wn) # transform to freq domain wnf /= (np.linspace(0.1,1,len(wnf))*5)**2 # spectrum ~ 1/f² sig = np.fft.irfft(wnf) # transform to time domain # vector with all time instances t = np.arange(self.numsamples, dtype=float) / self.sample_freq # iterate over all rotors for rpm in self.rpm_list: f_base = rpm / 60 # rotor speed in Hz # randomly set phase of rotor phase = rng.uniform() * 2*np.pi # calculate higher harmonics up to 50 times the rotor speed for n in np.arange(50)+1: # if we&amp;#39;re looking at a blade passing frequency, make it louder if n % self.num_blades_per_rotor == 0: amp = 1 else: amp = 0.2 # exponentially decrease amplitude for higher freqs with arbitrary factor amp *= np.exp(-n/10) # add harmonic signal component to existing signal sig += amp * np.sin(2*np.pi*n * f_base * t + phase) # return signal normalized to given RMS value return sig * self.rms / np.std(sig) Now we use the newly-defined class to create an object with parameters for a specific drone signal. We choose to have a quadcopter with four rotors. As the simulation should feature the drone flying by, it makes sense for two of the rotors running at a higher speed than the other two, so as to tilt a quadcopter to let it fly in one direction. Moreover, all rotors are chosen to have slightly different rotational speeds to make it a little more realistic (15010 rpm / 14962 rpm and 13536 rpm / 13007 rpm). The sample_freq and numsamples traits that we set here are inherited from the SignalGenerator base class. We use a standard sampling frequency for audio signals of 44'100 Hz here and generate a signal of a little above 10 seconds. The strength of the signal can be set via the (also inherited) rms trait, but we just use the default value of 1.0 here, so don't have to specifically set it. t_msm = 10.5 # s # sampling frequency f_sample = 44100 # Hz drone_signal = DroneSignalGenerator(rpm_list = [15010,14962,13536,13007], num_blades_per_rotor = 2, sample_freq = f_sample, numsamples = f_sample*t_msm) # If you&amp;#39;re running the example in an interactive environment, you might want # to listen to the pure signal by uncommenting the two following lines: #from IPython.display import Audio #display(Audio(drone_signal.signal(),rate = f_sample)) The spectrum (PSD) of our signal looks like this: plt.figure(1,(10,3)) plt.psd(drone_signal.signal(), Fs = f_sample, NFFT = 4096) plt.show() In addition to the signal, we need characteristics of the source itself, its movement, the environment, and the way it is observed. We start with the &quot;observer&quot;, i.e. a microphone array with only two microphones where the ears of a standing person might be located. The observer's head is looking mostly towards positive $y$, but also slightly to the side from which the drone will come later. m = ac.MicGeom() m.mpos_tot = np.array([[-0.07, 0.07], # x positions, all values in m [-0.03, 0.03], # y [ 1.7 , 1.7]]) # z Next, the flight path is defined. A trajectory in Acoular is defined via its &quot;waypoints&quot; with corresponding times. We can set as many waypoints as we want as a dictionary. Let's say that our drone flies about 10 m above ground, from left to right, and a little in front of the observer. The flight speed is 16 m/s in $x$ direction. The absolute speed will be a little higher, since we slightly and randomly vary the $z$ and $y$ positions from one waypoint to the next. flight_speed = 16 # m/s # 11 seconds trajectory, which is a little more than we have signal for ts = np.arange(12) # Set one waypoint each second, waypoints = { t : ((t-5.5)*flight_speed, # vary 6 + rng.uniform(-0.2,0.2), # randomly vary y position up to ±0.2 m around 6 m 10 + rng.uniform(-0.3,0.3)) # randomly vary z position up to ±0.3 m around 10 m height for t in ts } traj = ac.Trajectory(points = waypoints) Let's plot the trajectory together with the observer positions, as viewed from above. plt.figure(2,(10,3)) # plot observer plt.plot(m.mpos[0,:], m.mpos[1,:], &amp;#39;rx&amp;#39;, label = &amp;#39;observer&amp;#39;) # plot trajectory times = np.linspace(0,11,100) xt, yt, zt = traj.location(times) plt.plot(xt, yt, label = &amp;#39;trajectory&amp;#39;) # plot the predefined waypoints xwp, ywp, zwp = zip(*traj.points.values()) plt.plot(xwp, ywp, &amp;#39;&amp;gt;&amp;#39;, label = &amp;#39;traj. waypoints&amp;#39;) plt.xlabel(&amp;#39;$x$ / m&amp;#39;) plt.ylabel(&amp;#39;$y$ / m&amp;#39;) plt.legend() plt.axis(&amp;#39;equal&amp;#39;) plt.show() Compared to the length of the flight path, the two microphones are positioned rather close to each other, so they appear almost as one position in the plot. Now we define an actual source. Many drones exhibit a strong directivity, so using a MovingPointSourceDipole seems a good choice here. If we don't explicitly specify otherwise, the dipole lobes will be oriented along the z axis, which is what we want in this case. For calculating the observed sound pressure, i.e. for taking into account the sound travel path source to the observer, the object needs to know what kind of environment it will be existing in. In our case, we just define a resting fluid with a speed of sound of 343 m/s. e = ac.Environment(c=343.) # Define point source p = ac.MovingPointSourceDipole(signal = drone_signal, # the signal of the source trajectory = traj, # set trajectory conv_amp = True, # take into account convective amplification mics = m, # set the &amp;quot;array&amp;quot; with which to measure the sound field start = 0.5, # observation starts 0.5 seconds after signal starts at drone env = e) # the environment the source is moving in With this, we defined our source. But for a little more realism, let's add a &quot;mirror source&quot; to simulate ground reflections. The properties of this source are exactly the same as for the original source, except for the $z$ coordinate, which is inversed here. After the definition of our mirror source, both source are combined into a joint sound field description via a SourceMixer object. waypoints_reflection = { time : (x, y, -z) for time, (x, y, z) in waypoints.items() } traj_reflection = ac.Trajectory(points = waypoints_reflection) # Define a mirror source with the mirrored trajectory p_reflection = ac.MovingPointSourceDipole(signal = drone_signal, # the same signal as above trajectory = traj_reflection, # set trajectory of mirror source conv_amp = True, mics = m, start = 0.5, env = e) # Mix the original source and the mirror source drone_above_ground = ac.SourceMixer( sources = [p, p_reflection] ) Now we export a wav file of our simulation, so that we can listen to it with any audio player software. Because of the lazy evaluation paradigm, all the code above should run rather quickly, since no serious calculations were done up to now. This will change here, as the source signal is propagated sample-per-sample to generate the observed sound pressure time signals. The export may take a few minutes, so don't be alarmed if you don't immediately get the output. Before letting the data stream be exported as wav file, however, we divert it through a TimeCache object, which already writes the data on the disk (in Acoular-compatible format). The reason for this is that we do not need to recalculate everything if we happen to need the exact same data at a later point in time. cached_signals = ac.TimeCache(source = drone_above_ground) # Prepare wav output. # If you don&amp;#39;t need caching, you can directly put &amp;quot;source = drone_above_ground&amp;quot; here. output = ac.WriteWAV(name = &amp;#39;drone_flyby_with_ground_reflection.wav&amp;#39;, source = cached_signals, channels = [0,1]) # export both channels as stereo # Start the actual export output.save() That's it, we did a simple drone auralization using the tools available in Acoular. If you listen to the output file with properly connected stereo headphones, you should hear something that resembles a quadcopter moving from left to right. Finally, let's visualize the transient signal of one channel in a spectrogram. We use Acoular's return_result function on the output object to get the whole signal track at once instead of from a block-wise yielding generator. plt.figure(3,(10,5)) plt.specgram(ac.tools.return_result(output)[:,0], Fs = f_sample, noverlap = 4096-256, NFFT = 4096, vmin=-100, vmax=-50) plt.ylim(0,5000) plt.colorbar() plt.xlabel(&amp;#39;$t$ / s&amp;#39;) plt.ylabel(&amp;#39;$f$ / Hz&amp;#39;) plt.show() Although this simulation is comparativly simple and far from realistic, the spectrogram already exhibits many of the features observed in actual measurement data of drone flybys: the frequency shift due to the Doppler effect, higher levels when the drone is close to the observer, and interference patterns due to ground reflections.</summary></entry><entry><title type="html">How to import your data into Acoular</title><link href="https://blog.acoular.org/2022/06/24/convert-input-data.html" rel="alternate" type="text/html" title="How to import your data into Acoular" /><published>2022-06-24T00:00:00-05:00</published><updated>2022-06-24T00:00:00-05:00</updated><id>https://blog.acoular.org/2022/06/24/convert-input-data</id><author><name>Ennes Sarradj</name></author><category term="data import" /><summary type="html">Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array which is stored in an HDF5 file. This blog post explains how to convert data available in other formats into this file format. As examples for other file formats we will use both .csv (comma separated text files) and .mat (Matlab files).</summary></entry><entry><title type="html">Getting started with Acoular - Part 3</title><link href="https://blog.acoular.org/2021/04/01/getstart3.html" rel="alternate" type="text/html" title="Getting started with Acoular - Part 3" /><published>2021-04-01T00:00:00-05:00</published><updated>2021-04-01T00:00:00-05:00</updated><id>https://blog.acoular.org/2021/04/01/getstart3</id><author><name>Ennes Sarradj</name></author><category term="time domain beamforming" /><summary type="html">This is the third and final in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first two posts and continues by explaining additional concepts to be used with time domain methods. Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources. To continue, we do the same set up as in Part 1. However, as we are setting out to do some signal processing in time domain, we define only TimeSamples, MicGeom, RectGrid and SteeringVector objects but no PowerSpectra or BeamformerBase. import acoular ts = acoular.TimeSamples( name=&amp;quot;three_sources.h5&amp;quot; ) mg = acoular.MicGeom( from_file=&amp;quot;array_64.xml&amp;quot; ) rg = acoular.RectGrid( x_min=-0.2, x_max=0.2, y_min=-0.2, y_max=0.2, z=0.3, increment=0.01 ) st = acoular.SteeringVector( grid=rg, mics=mg ) For processing in time domain in Acoular, one may set up &quot;chains&quot; of processing blocks. This is very flexible and allows for easy implementation of new algorithms or algorithmic steps. Each of the blocks acts on all channels at once. Input and output may have different numbers of channels. For our task we set up the following processing chain: data intake from file (TimeSamples, same as before) beamforming. In the time domain this amounts to different delays that have to be applied to all channels and for all grid points, and a sum for each of the grid points. This is also known as delay-and-sum. band pass filtering (the time history for each point in the map is filtered). We could skip that step in principle, but it is nice to compare the result to what we got in Parts 1 and 2 from frequency domain processing, where we did a similar approach to band pass filtering. power estimation (just the square, nothing else), so that we can compute levels linear average over consecutive blocks in time which makes it possible to have not one image for every sample in time, which is huge amount of (mostly useless) data, but just enough data for some images Each object in the processing chain is connected to its predecessor via the source parameter: bt = acoular.BeamformerTime( source=ts, steer=st ) ft = acoular.FiltOctave( source=bt, band=8000, fraction=&amp;#39;Third octave&amp;#39; ) pt = acoular.TimePower( source=ft ) avgt = acoular.TimeAverage( source=pt, naverage=6400 ) And again: lazy evaluation, nothing is computed yet. Only asking for the result will initiate computing. Although this is not used in this example, it should be mentioned that the architecture allows for endless data processing from a stream of input data. To this end it is possible to replace the TimeSamples object that reads the data not from a file, but from hardware. Different to the frequency domain processing, the result is not computed in one go, but in blocks of data. These blocks have a variable length that can be defined as argument of the result function each of the processing blocks have. Note that this function is actually a Python generator, that yields a number of results we have to iterate over. This helps if one wants to process large amounts of data that do not fit into the memory at once. Iteration means we can use it in for loop and get a new data block each time we run through the loop. In our example we use the loop in a Python list comprehension statement. That means we collect all blocks into a list. In our case, the blocks have length 1, i.e. one map per block. res = [r.copy() for r in avgt.result(1)] The list res contains all maps each of which averages over 6400 samples. Now we can plot all of these maps. Because time domain processing sees the map as (number of gridpoints) channels, we have to reshape the maps so that fit the shape of the grid. %matplotlib notebook import matplotlib.pyplot as plt plt.figure(figsize=(10,7)) for i,r in enumerate(res): pm = r[0].reshape(rg.shape) Lm = acoular.L_p(pm) plt.subplot(2,4,i+1) plt.imshow(Lm.T, vmin=Lm.max()-15, origin=&amp;#39;lower&amp;#39;, extent=rg.extend()) plt.title(&amp;#39;sample %i to %i&amp;#39; % (i*6400,i*6400+6399)) plt.tight_layout(); Because the sources emit a stationary signal, the individual maps do look not much different. The result is also very similar to what we got with the frequency domain beamformer in Part 1. We can assemble the processing chain also in a different way with positions 2 and 3 exchanged: data intake band pass filtering beamforming power estimation linear average In this case we have to be careful about the effects of filtering: Because the band pass filter comes also with a frequency dependent delay, this can disturb the work of the beamformer in case that sources are not stationary. To circumvent this, we could use a special filter FiltFiltOctave with zero delay. The disavantage of this filter is that the whole time history must be read into the memory, before the first block can be processed by the beamformer. In the present simple example it is not necessary to do this. We just &quot;rewire&quot; the processing chain. One advantage in this case is that we only have to band pass filter 64 channels (number of microphones) instead of 1641 channels (number of grid points as in the first case. ft.source = ts bt.source = ft pt.source = bt After the new chain is set up, we can again plot the results. This time we do not use an extra list for all results, but iterate directly over the maps while plotting. Remember that this takes some time. plt.figure(figsize=(10,7)) for i,r in enumerate(avgt.result(1)): pm = r[0].reshape(rg.shape) Lm = acoular.L_p(pm) plt.subplot(2,4,i+1) plt.imshow(Lm.T, vmin=Lm.max()-15, origin=&amp;#39;lower&amp;#39;, extent=rg.extend()) plt.title(&amp;#39;sample %i to %i&amp;#39; % (i*6400,i*6400+6399)) plt.tight_layout(); According to our expectations, the result looks very much the same with this alternative processing chain. This concludes this series of three blog posts about first steps in Acoular. If you want to know more, look at the documentation and the examples and the reference you will find there or watch out for new blog posts to come!</summary></entry><entry><title type="html">Getting started with Acoular - Part 2</title><link href="https://blog.acoular.org/2021/04/01/getstart2.html" rel="alternate" type="text/html" title="Getting started with Acoular - Part 2" /><published>2021-04-01T00:00:00-05:00</published><updated>2021-04-01T00:00:00-05:00</updated><id>https://blog.acoular.org/2021/04/01/getstart2</id><author><name>Ennes Sarradj</name></author><category term="frequency domain beamforming" /><category term="deconvolution" /><category term="Clean SC" /><category term="functional beamforming" /><summary type="html">This is the second in a series of three blog posts about the basic use of Acoular. It assumes that you already have read the first post and continues by explaining some more concepts and additional methods. Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources.</summary></entry><entry><title type="html">Getting started with Acoular - Part 1</title><link href="https://blog.acoular.org/2021/04/01/getstart1.html" rel="alternate" type="text/html" title="Getting started with Acoular - Part 1" /><published>2021-04-01T00:00:00-05:00</published><updated>2021-04-01T00:00:00-05:00</updated><id>https://blog.acoular.org/2021/04/01/getstart1</id><author><name>Ennes Sarradj</name></author><category term="frequency domain beamforming" /><summary type="html">This is the first in a series of three blog posts about the basic use of Acoular. It explains some fundamental concepts and walks through a simple example. Acoular is a Python library that processes multichannel data (up to a few hundred channels) from acoustic measurements with a microphone array. The focus of the processing is on the construction of a map of acoustic sources. This is somewhat similar to taking an acoustic photograph of some sound sources.</summary></entry></feed>